{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I tried a wide variety of models for this competition. The caret framework makes it very easy to test different models in R. Some of the models that I tried are:\n",
    "\n",
    "* Random Forest\n",
    "* Extremely Randomized Trees\n",
    "* Extreme Gradient Boosted Trees (XGBoost)\n",
    "* Elastic Nets\n",
    "* Generalized Linear Models\n",
    "* Generalized Additive Models\n",
    "* Online Linear Learning (via Vowpal Wabbit)\n",
    "\n",
    "The nice thing about the caret package is that it provides a unified interface for these models. Of these models, I found that XGBoost had the most predictive power. Once I settled on XGBoost, I just ran XGBoost directly without the caret wrapper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transformation of the Dependent Variable\n",
    "\n",
    "The demand variable should be log transformed. There are a few reasons for this:\n",
    "\n",
    "1. The metric for the competition is RMSLE (Root Mean Square Log Error). Most methods support the RMSE (Root Mean Square Error) metric. By log transformation, the RMSE metric becomes equivalent to the RMSLE metric\n",
    "2. Demand is always a positive number. By log transformation, we restrict the demand to be a positive number on the inverse transformation. An alternate way of saying this is that most methods assume that the range of the dependent variable is from -Inf to +Inf.\n",
    "\n",
    "I modified the formatData() function to support a logTransform parameter for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘xgboost’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    slice\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library (plyr)\n",
    "library (dplyr)\n",
    "library (ggplot2)\n",
    "library (lubridate)\n",
    "library (reshape2)\n",
    "library (xgboost)\n",
    "\n",
    "source (\"support.R\")\n",
    "train.df <- read.csv (\"data/train.csv\")\n",
    "test.df <- read.csv (\"data/test.csv\")\n",
    "train.df <- formatData (train.df, logTransform=TRUE) %>% tbl_df()\n",
    "\n",
    "test.df <- formatData (test.df) %>% tbl_df()\n",
    "\n",
    "train.df$month <- factor (train.df$month)\n",
    "train.df$year <- factor (train.df$year)\n",
    "\n",
    "test.df$month <- factor (test.df$month)\n",
    "test.df$year <- factor (test.df$year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I've converted the month and year variables to a factor. The reason I did this was that I didn't see any continuous behavior in demand as a result of these two variables. \n",
    "\n",
    "We are now ready to run the XGBoost model. At this point, we have a choice to make. Either we can predict total demand or we could predict casual and regular demand separately and then sum them. From my explorations, predicting casual and regular demand separately yielded a slightly higher score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Casual Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.formula <-  ~ season + holiday + workingday + weather + temp + atemp +\n",
    "        humidity + windspeed + year + month + wday + day + hour - 1\n",
    "\n",
    "trainData <- xgb.DMatrix (model.matrix (train.formula, train.df), label=train.df$casual)\n",
    "testData <- xgb.DMatrix (model.matrix (train.formula, test.df))\n",
    "\n",
    "set.seed (4322)\n",
    "\n",
    "#The parameters below are optimal parameters, calculated via hyperparameter optimization using the hyperopt package\n",
    "#I'll show my setup to use this package a bit later. If you want to take a quick peek, look at the hyperopt/ directory\n",
    "params <- list (booster=\"gbtree\",\n",
    "                eta=0.00330925962444,\n",
    "                gamma=0.684530964272,\n",
    "                max_depth=7,\n",
    "                min_child_weight=0.596497397942,\n",
    "                subsample=0.678093555386,\n",
    "                colsample_bytree=0.662176894972,\n",
    "                objective=\"reg:linear\",\n",
    "                eval_metric=\"rmse\")\n",
    "\n",
    "fit1 <- xgb.train (params, trainData, nround = 8500, nfold = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Registered Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData <- xgb.DMatrix (model.matrix (train.formula, train.df), label=train.df$registered)\n",
    "\n",
    "params <- list (booster=\"gbtree\",\n",
    "                eta=0.00291713063475,\n",
    "                gamma=0.00833471795637,\n",
    "                max_depth=6,\n",
    "                min_child_weight=1.57952698042,\n",
    "                subsample=0.626763785155,\n",
    "                colsample_bytree=0.685032802413,\n",
    "                objective=\"reg:linear\",\n",
    "                eval_metric=\"rmse\")\n",
    "\n",
    "fit2 <- xgb.train (params, trainData, nround = 10000, nfold = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.pred <- (exp (predict (fit1, testData)) - 1) + (exp (predict (fit2, testData)) - 1)\n",
    "y.pred[y.pred < 0] = 0\n",
    "result.df <- data.frame (datetime=strftime (test.df$datetime, \n",
    "                                            format=\"%Y-%m-%d %H:%M:%S\", \n",
    "                                            tz=\"UTC\"),\n",
    "                         count=y.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This single optimized XGBoost model will result in 97th place. Not bad, eh?\n",
    "\n",
    "![97th Place](files/images/xgb-separate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, I decided that I really wanted a top 50 finish. So I set about trying to improve this model.\n",
    "\n",
    "#### How I actually accomplished this is a story for a different day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
